âœ¨ Key Features

ğŸ“„ PDF Upload & Processing
Extract text, intelligently chunk it with overlap, generate embeddings, and store vectors in Qdrant for semantic search.
ğŸ’¬ Contextâ€‘Aware Chat
Ask questions and receive answers grounded in your documents. The system retrieves relevant chunks, optionally adds web search results, and queries a Neo4j knowledge graph for related facts.
ğŸŒ Optional Web Search
Integrates with Tavily to fetch upâ€‘toâ€‘date information when needed.
ğŸ§  Knowledge Graph
Conversations and extracted entities are stored in Neo4j, enabling relationshipâ€‘based context over time.
ğŸ” Authentication
Userâ€‘specific data isolation via NextAuth.js â€“ every user sees only their own documents and history.
âš¡ Modern Stack
Built with Next.js, TypeScript, Tailwind CSS, and deployed on Vercelâ€‘ready architecture.
ğŸ› ï¸ Tech Stack

Frontend & API: Next.js 
Auth: NextAuth.js
Vector Database: Qdrant
Graph Database: Neo4j
LLM Provider: Cerebras (llama-3.3-70b)
Embeddings: OpenAI / compatible (configurable)
Web Search: Tavily API
PDF Parsing: pdf-parse
ğŸš€ How It Works

Upload a PDF â€“ the file is chunked, embedded, and stored in Qdrant; metadata goes to Neo4j.
Chat â€“ your question is used to retrieve relevant document chunks from Qdrant, optionally augmented with web search and knowledge graph data.
Generate â€“ the combined context is sent to Cerebras, which produces a fluent, sourced answer.
Remember â€“ every conversation is saved in Neo4j, building a knowledge graph over time.
